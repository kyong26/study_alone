# 파이썬 텍스트 마이닝 완벽 가이드

## 1부. 텍스트 마이닝 기초

### 02. 텍스트 전처리

* 추가 참고 사이트: https://datascienceschool.net/03%20machine%20learning/03.01.01%20NLTK%20자연어%20처리%20패키지.html

### 2.1 텍스트 전처리의 개념

- 자연어 처리의 단계\
(1) 자연어로 쓰여진 글을 전처리 하는 준비단계(공통적)  
(2) 전처리된 결과를 컴퓨터가 다루고 이해할 수 있는 형태로 변환하는 단계(분석의 목적과 방법론에 따라 달라짐)  
(3) 변환된 형태를 이용해 다양한 분석을 수행하는 단계

&nbsp;

- 텍스트 전처리: 주어진 텍스트에서 노이즈와 같이 불필요한 부분을 제거하고, 문장을 표준 단어들로 분리한 후에 각 단어의 품사를 파악하는 것
- 텍스트 전처리의 단계  
(1) 정제: 불필요한 노이즈를 제거  
(2) 토큰화: 주어진 텍스트를 원하는 단위로 나누는 작업  
(3) 정규화: 같은 의미를 가진 동일한 단어임에도 불구하고 다른 형태로 쓰여진 단어들을 통일시켜서 표준 단어로 만드는 작업(어간 추출, 표제어 추출)  
(4) 품사태깅: 토큰화 한 단어에 대해 품사를 파악해 부착하는 것


&nbsp;

### 2.2 토큰화


```python
# 실습을 위한 nltk 라이브러리 다운로드

import nltk
nltk.download('punkt')
nltk.download('webtext')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
```

    [nltk_data] Downloading package punkt to
    [nltk_data]     /Users/seomingyeong/nltk_data...
    [nltk_data]   Unzipping tokenizers/punkt.zip.
    [nltk_data] Downloading package webtext to
    [nltk_data]     /Users/seomingyeong/nltk_data...
    [nltk_data]   Unzipping corpora/webtext.zip.
    [nltk_data] Downloading package wordnet to
    [nltk_data]     /Users/seomingyeong/nltk_data...
    [nltk_data]   Unzipping corpora/wordnet.zip.
    [nltk_data] Downloading package stopwords to
    [nltk_data]     /Users/seomingyeong/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.
    [nltk_data] Downloading package averaged_perceptron_tagger to
    [nltk_data]     /Users/seomingyeong/nltk_data...
    [nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.





    True



### 문장토큰화


```python
para = "Hello everyone. It's good to see you. Let's start our text mining class!"

# 영어학습에 사전학습된 모델을 사용해 문장을 토큰화 한다
from nltk.tokenize import sent_tokenize

# 주어진 텍스트를 문장 단위로 토큰화. 주로 . ! ? 등을 이용
print(sent_tokenize((para)))
```

    ['Hello everyone.', "It's good to see you.", "Let's start our text mining class!"]



```python
# 프랑스어를 학습한 모델 사용
paragraph_french = """Je t'ai demand si tu m'aimais bien, Tu m'a r pondu non.
Je t'ai demand si j' trais jolie, Tu m'a r pondu non.
Je t'ai demand si j' tai dans ton coeur, Tu m'a r pondu non."""

import nltk.data
tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')
print(tokenizer.tokenize(paragraph_french))
```

    ["Je t'ai demand si tu m'aimais bien, Tu m'a r pondu non.", "Je t'ai demand si j' trais jolie, Tu m'a r pondu non.", "Je t'ai demand si j' tai dans ton coeur, Tu m'a r pondu non."]



```python
para_kor = "안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다."

print(sent_tokenize(para_kor))
```

    ['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다.']


### 단어토큰화


```python
from nltk.tokenize import word_tokenize
print(word_tokenize(para))
```

    ['Hello', 'everyone', '.', 'It', "'s", 'good', 'to', 'see', 'you', '.', 'Let', "'s", 'start', 'our', 'text', 'mining', 'class', '!']



```python
from nltk.tokenize import WordPunctTokenizer
print(WordPunctTokenizer().tokenize(para))
```

    ['Hello', 'everyone', '.', 'It', "'", 's', 'good', 'to', 'see', 'you', '.', 'Let', "'", 's', 'start', 'our', 'text', 'mining', 'class', '!']



```python
print(word_tokenize(para_kor))
```

    ['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '.']


### 정규식을 이용한 토큰화

* 참고  
https://regexr.com  
https://wikidocs.net/4308


```python
import re
re.findall("[abc]", "how are you, boy?")
```




    ['a', 'b']




```python
# 숫자 찾기
re.findall("[0-9]", "3A7b5c_*9D")
```




    ['3', '7', '5', '9']




```python
# 알파벳 찾기
re.findall("[A-Za-z]", "3A7b5c_*9D")
```




    ['A', 'b', 'c', 'D']




```python
# 알파벳, 숫자, '_' 찾기
re.findall("[\w]", "3A7b5c_*9D")
```




    ['3', 'A', '7', 'b', '5', 'c', '_', '9', 'D']




```python
# 메타문자 '+', 한 번 이상 반복
re.findall("[_]+", "a_b, c__d, e___f")
```




    ['_', '__', '___']




```python
# \w에 공백이 포함되지 않아, 문자열에서 공백이나 쉼표로 구분되는 단어들을 쪼갤 수 있음
print(re.findall("[\w]", "How are you, boy?"))
print(re.findall("[\w]+", "How are you, boy?"))
```

    ['H', 'o', 'w', 'a', 'r', 'e', 'y', 'o', 'u', 'b', 'o', 'y']
    ['How', 'are', 'you', 'boy']



```python
# {}: 반복 횟수를 지정
# 2~4회 반복된 문자열 찾기
re.findall("[o]{2,4}", "oh, hoow are yoooou, boooooooy?")
```




    ['oo', 'oooo', 'oooo', 'ooo']




```python
from nltk.tokenize import RegexpTokenizer

# regular expression(정규식)을 이용한 Tokenizer
# 단어 단위로 tokenize \w:문자나 숫자를 의미. 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아냄

tokenizer = RegexpTokenizer("[\w']+")
print(tokenizer.tokenize("Sorry, I can't go there."))
```

    ['Sorry', 'I', "can't", 'go', 'there']



```python
# 텍스트를 모두 소문자로 바꾸고, '를 포함해 세 글자 이상의 단어들만 골라냄
text1 = "Sorry, I can't go there."
tokenizer = RegexpTokenizer("[\w']{3,}")
print(tokenizer.tokenize(text1.lower()))
```

    ['sorry', "can't", 'there']


### 노이즈와 불용어 제거

- 영어에서는 보통 길이가 3 미만인 단어들은 삭제하는 것이 일반적
- NLTK에서는 stopwords라는 라이브러리를 이용해 언어별 불용어 사전을 제공


```python
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들
english_stops = set(stopwords.words('english')) # 반복되지 않도록 set으로 전환

text1 = "Sorry, I couldn't go to movie yesterday"

tokenizer = RegexpTokenizer("[\w']+")
tokens = tokenizer.tokenize(text1.lower()) # word_tokenizer로 토큰화

# stopwords를 제외한 단어들만으로 list를 생성
result = [word for word in tokens if word not in english_stops]

print(result)
```

    ['sorry', 'go', 'movie', 'yesterday']



```python
print(english_stops)
```

    {'the', 'couldn', 'other', "shan't", 'again', 'he', "it's", 'to', 'same', 'you', 'yourselves', "she's", 'which', 'by', 'those', 'some', "doesn't", 'during', 'ma', 'against', 'after', 'their', 'be', 'its', 'this', 'at', 'my', 'i', 'for', 'does', "didn't", 'as', 'here', 'of', "couldn't", 'haven', "weren't", 'she', 'these', 'yours', 'in', 'under', 'and', 'most', 'was', 'up', "wasn't", 'we', "don't", 'through', 'doesn', 're', 'more', 'than', 'am', 'both', 'didn', "needn't", 'ourselves', 'now', 'm', 'whom', 'very', 'before', 'above', 'off', 'over', 'hadn', "isn't", "you'll", 'mightn', 'wouldn', 'her', 'doing', 'below', 'it', "hasn't", 'all', 'our', 'nor', "won't", 'about', 'his', 'were', 'hasn', 'why', 'any', 'that', "aren't", 'not', 'who', 'a', 'should', 'is', 'they', 've', 'have', 'because', "you'd", 'aren', 'can', "wouldn't", "haven't", 'such', 'are', 'shouldn', 'so', 'but', 'herself', 'needn', 'once', 'weren', 'themselves', "you're", 'until', 'me', 'do', "that'll", 'y', 'don', 'on', 'no', "shouldn't", 'then', 'there', 'ain', 'few', 'your', "should've", 'mustn', 'further', 'myself', 'when', 'yourself', 'had', 'what', "mightn't", 'out', 'or', 'has', 'ours', 't', 'him', 'with', 'too', "hadn't", 'into', 'how', 'own', 'only', 'll', 'an', 's', 'o', 'having', 'from', 'd', "you've", 'isn', 'shan', 'theirs', 'if', 'hers', 'itself', 'down', 'where', 'himself', 'being', 'did', 'been', 'while', 'will', 'just', "mustn't", 'them', 'each', 'wasn', 'between', 'won'}



```python
# 자신만의 stopwords를 만들고 이용
# 한글처리에도 유용함
# 나만의 stopword를 리스트로 정리

my_stopword = ['i','go','to']

result = [word for word in tokens if word not in my_stopword]
print(result)
```

    ['sorry', "couldn't", 'movie', 'yesterday']


### 2.3 정규화

- 정규화: 같은 의미를 가진 동일한 단어이면서 다른 형태로 쓰여진 단어들을 통일해 표준 단어로 만드는 작업
- 어형: 단어의 형태
- 용언: 문장 안에서 서술하는 구실을 하는 동사와 형용사
- 어간(stem): 어형변화에서 변화하지 않는 부분, 용언의 바뀌지 않는 부분
- 어미: 용언의 바뀌는 부분

&nbsp;

### 어간 추출
- 영어의 경우에는 명사가 복수형으로 기술된 것을 단수형으로 바꾸는 작업도 어간 추출에 포함됨
- 영어 어간추출 알고리즘: 포터 스테머(Poter Stemmer), 랭카스터 스테머(Lancaster Stemmer)


```python
# 포터 스테머

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))

# 단어가 변형되는 규칙을 이용해 원형을 찾으므로, 그 결과가 항상 사전에 있는 올바른 단어가 되지는 않는다.
# 중요한 것은 포터 스테머를 쓰면 모든 단어가 같은 규칙에 따라 변환된다는 것이다.
# 즉 변환된 단어가 올바른 단어가 아니더라도, 동일한 형태로 변환됐으므로 분석의 의도를 충족시킬 수 있다.
```

    cook cookeri cookbook



```python
# 토큰화와 결합해 어간을 추출
from nltk.tokenize import word_tokenize

para = "Hello everyone. It's good to see you. Let's start our text mining class!"
tokens = word_tokenize(para)
print(tokens)

result = [stemmer.stem(token) for token in tokens]
print(result)
```

    ['Hello', 'everyone', '.', 'It', "'s", 'good', 'to', 'see', 'you', '.', 'Let', "'s", 'start', 'our', 'text', 'mining', 'class', '!']
    ['hello', 'everyon', '.', 'it', "'s", 'good', 'to', 'see', 'you', '.', 'let', "'s", 'start', 'our', 'text', 'mine', 'class', '!']



```python
# 랭카스터 스테머

from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()
print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))
```

    cook cookery cookbook


### 표제어 추출(Lemmatization)

- lemma: 단어의 기본형, 사전에 나오는 단어  
(예) '작다'의 '작'은 어간, 표제어는 '작다'


```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize('cooking'))
print(lemmatizer.lemmatize('cooking', pos='v')) # 품사를 지정
print(lemmatizer.lemmatize('cookery'))
print(lemmatizer.lemmatize('cookbooks'))
```

    cooking
    cook
    cookery
    cookbook


### 2.4 품사 태깅

- 형태소: 토큰화와 정규화를 거쳐서 나온 각 결과, 의미를 가진 가장 작은 말의 단위, 더 나누게 되면 본래의 뜻을 잃어버린다.
- 품사: 명사, 대명사, 수사, 조사, 동사, 형용사, 관형사, 부사, 감탄사와 같이 공통된 성질을 지닌 낱말끼리 모아 놓은 낱말의 갈래.
- 낱말: 뜻을 가지고 홀로 쓰일 수 있는 말의 가장 작은 단위

&nbsp;

단위

1. 음절: 하나의 종합된 음의 느낌을 주는 말소리의 단위
2. 형태소: 뜻을 가진 가장 작은 말의 단위(주로 토큰화 하는 단위이다)
3. 어절: 문장을 구성하는 각각의 마디, 띄어쓰기의 단위


&nbsp;

#### 주요품사
- 명사: 이름을 나타내는 낱말
- 대명사: 이름을 대신해 가리키는 낱말
- 수사: 수량이나 순서를 가리키는 낱말
- 조사: 도와주는 낱말
- 동사: 움직임을 나타내는 낱말
- 형용사: 상태나 성질을 나타내는 낱말
- 관형사: 체언을 꾸며 주는 낱말
- 부사: 주로 용언을 꾸며 주는 낱말
- 감탄사: 놀람, 느낌, 부름, 대답을 나타내는 낱말

&nbsp;

- 용언: 동사와 형용사를 함께 부르는 말
- 체언: 명사, 대명사, 수사를 묶어서 부르는 말
- 수식언: 관형사와 부사를 묶어서 부르는 말
- 관계언: 조사
- 독립언: 감탄사
- 품사 태깅(Part-of-Speech Tagging): 형태소에 대해 품사를 파악해 부착(tagging)하는 작업


```python
# NLTK를 활용한 품사 태깅
import nltk
from nltk.tokenize import word_tokenize

tokens = word_tokenize("Hello everyone. It's good to see you. Let's start our text-mining class!")
print(nltk.pos_tag(tokens))
```

    [('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), ("'s", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), ("'s", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text-mining', 'JJ'), ('class', 'NN'), ('!', '.')]



```python
nltk.download()
```

    showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml





    True




```python
nltk.help.upenn_tagset("VB")
```

    VB: verb, base form
        ask assemble assess assign assume atone attention avoid bake balkanize
        bank begin behold believe bend benefit bevel beware bless boil bomb
        boost brace break bring broil brush build ...



```python
nltk.help.upenn_tagset('CC')
```

    CC: conjunction, coordinating
        & 'n and both but either et for less minus neither nor or plus so
        therefore times v. versus vs. whether yet



```python
# 원하는 품사의 단어들만 추출
my_tag_set = ['NN', 'VB', 'JJ']
my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]
print(my_words)
```

    ['everyone', 'good', 'see', 'Let', 'start', 'text-mining', 'class']



```python
# 단어에 품사 정보를 추가해 구분
words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]
print(words_with_tag)
```

    ['Hello/NNP', 'everyone/NN', './.', 'It/PRP', "'s/VBZ", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', "'s/POS", 'start/VB', 'our/PRP$', 'text-mining/JJ', 'class/NN', '!/.']



```python
sentence = '''절망의 반대가 희망은 아니다.
어두운 밤하늘에 별이 빛나도
희망은 절망 속에 싹트는 거지
만약에ㅔ 우리가 희망함이 적다면
그 누가 세상을 비추어줄까
정희성, 희망 공부'''

from konlpy.tag import Okt
t = Okt()

print('형태소', t.morphs(sentence))
print()
print('명사', t.nouns(sentence))
print()
print('품사 태깅 결과', t.pos(sentence))
```

    형태소 ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나도', '\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\n', '만약', '에', 'ㅔ', '우리', '가', '희망', '함', '이', '적다면', '\n', '그', '누가', '세상', '을', '비추어줄까', '\n', '정희성', ',', '희망', '공부']

    명사 ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']

    품사 태깅 결과 [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나도', 'Verb'), ('\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('ㅔ', 'KoreanParticle'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비추어줄까', 'Verb'), ('\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]
