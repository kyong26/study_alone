# 파이썬 텍스트 마이닝 완벽 가이드

## 1부. 텍스트 마이닝 기초

### 03. 그래프와 워드 클라우드

### 3.1 단어 빈도 그래프


```python
import nltk
nltk.download('gutenberg')

from nltk.corpus import gutenberg
file_names = gutenberg.fileids()

print(file_names)
```

    ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
    

    [nltk_data] Downloading package gutenberg to
    [nltk_data]     C:\Users\xnsk2\AppData\Roaming\nltk_data...
    [nltk_data]   Package gutenberg is already up-to-date!
    


```python
doc_hamlet = gutenberg.open('shakespeare-hamlet.txt').read()
print('#Num of characters used:', len(doc_hamlet)) # 사용된 문자의 수 
print('#Text sample:')
print(doc_hamlet[:500]) # 앞의 500자만 출력
```

    #Num of characters used: 162881
    #Text sample:
    [The Tragedie of Hamlet by William Shakespeare 1599]
    
    
    Actus Primus. Scoena Prima.
    
    Enter Barnardo and Francisco two Centinels.
    
      Barnardo. Who's there?
      Fran. Nay answer me: Stand & vnfold
    your selfe
    
       Bar. Long liue the King
    
       Fran. Barnardo?
      Bar. He
    
       Fran. You come most carefully vpon your houre
    
       Bar. 'Tis now strook twelue, get thee to bed Francisco
    
       Fran. For this releefe much thankes: 'Tis bitter cold,
    And I am sicke at heart
    
       Barn. Haue you had quiet Guard?
      Fran. Not
    


```python
from nltk.tokenize import word_tokenize
tokens_hamlet = word_tokenize(doc_hamlet) # 토큰화 실행

print("#Nums of tokens used:", len(tokens_hamlet))
print('#Token sample:')
print(tokens_hamlet[:20])
```

    #Nums of tokens used: 36372
    #Token sample:
    ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']
    


```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

# 모든 토큰에 대해 스테밍 실행
stem_tokens_hamlet = [stemmer.stem(token) for token in tokens_hamlet]

print("#Num of tokens after stemming:", len(stem_tokens_hampet))
print('#Token sample')
print(stem_tokens_hamlet[:20])
```

    #Num of tokens after stemming: 36372
    #Token sample
    ['[', 'the', 'tragedi', 'of', 'hamlet', 'by', 'william', 'shakespear', '1599', ']', 'actu', 'primu', '.', 'scoena', 'prima', '.', 'enter', 'barnardo', 'and', 'francisco']
    


```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# 모든 토큰에 대해 스테밍 실행
lem_tokens_hamlet = [lemmatizer.lemmatize(token) for token in tokens_hamlet]

print('#Num of tokens after lemmatization:', len(lem_tokens_hamlet))
print('#Token sample:')
print(lem_tokens_hamlet[:20])

# 어간 추출이든 표제어 추출이든 토큰 수는 변하지 않는다.
# 토큰화한 결과에 따라 개별적으로 어간 추출과 표제어 추출을 수행하기 때문이다.
```

    #Num of tokens after lemmatization: 36372
    #Token sample:
    ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']
    


```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w]{3,}")

reg_tokens_hamlet = tokenizer.tokenize(doc_hamlet.lower())
print('#Num of tokens with RegexTokenizer:', len(reg_tokens_hamlet))
print('#Token sample:')
print(reg_tokens_hamlet[:20])

# 부호와 2자 이하의 단어가 제외되어 토큰 개수가 줄었다.
# 부호는 목적에 따라 의미를 가질 수도 있고 아닐 수도 있어, 부호를 포함할지는 텍스트 마이닝 수행자에 달려 있다.
# RegexpTokenizer 사용, 시각화 결과 이해를 위해 스테밍은 하지 않기로 함
```

    #Num of tokens with RegexTokenizer: 23187
    #Token sample:
    ['the', 'tragedie', 'hamlet', 'william', 'shakespeare', '1599', 'actus', 'primus', 'scoena', 'prima', 'enter', 'barnardo', 'and', 'francisco', 'two', 'centinels', 'barnardo', 'who', 'there', 'fran']
    


```python
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들
english_stops = set(stopwords.words('english')) # 반복되지 않도록 set으로 변환

# stopwords를 제외한 단어들로만 리스트를 생성
result_hamlet = [word for word in reg_tokens_hamlet if word not in english_stops]

print('#Num of tokens after stopword elimination:', len(result_hamlet))
print('#Token sample:')
print(result_hamlet[:20])
```
    #Num of tokens after stopword elimination: 15410
    #Token sample:
    ['tragedie', 'hamlet', 'william', 'shakespeare', '1599', 'actus', 'primus', 'scoena', 'prima', 'enter', 'barnardo', 'francisco', 'two', 'centinels', 'barnardo', 'fran', 'nay', 'answer', 'stand', 'vnfold']

```python
hamlet_word_count = dict()
for word in result_hamlet:
    hamlet_word_count[word] = hamlet_word_count.get(word,0)+1
    
print('#Num of used words:', len(hamlet_word_count))

sorted_word_count = sorted(hamlet_word_count, key=hamlet_word_count.get, reverse=True)

print('#Top 20 high frequency words:')
for key in sorted_word_count[:20]: # 빈도수 상위 20개의 단어를 출력
    print(f'{repr(key)}: {hamlet_word_count[key]}', end=',')
```

    #Num of used words: 4561
    #Top 20 high frequency words:
    'ham': 337,'lord': 211,'haue': 178,'king': 172,'thou': 107,'shall': 107,'come': 104,'let': 104,'hamlet': 100,'good': 98,'hor': 95,'thy': 90,'enter': 85,'like': 80,'would': 73,'well': 71,'know': 71,'tis': 69,'selfe': 68,'loue': 66,



```python
my_tag_set = ['NN','VB','VBD','JJ']
my_words = [word for word, tag in nltk.pos_tag(result_hamlet) if tag in my_tag_set]

hamlet_word_count = dict()
for word in my_words:
    hamlet_word_count[word] = hamlet_word_count.get(word, 0)+1
    
print('#Num of used words:', len(hamlet_word_count))

sorted_word_count = sorted(hamlet_word_count, key=hamlet_word_count.get, reverse=True) #dictionary 값으로 sorting

print('#Top 20 high frequency words:')
for key in sorted_word_count[:20]: # 빈도수 상위 20개의 단어를 출력
    print(f'{repr(key)}: {hamlet_word_count[key]}', end=',')
```

    #Num of used words: 3114
    #Top 20 high frequency words:
    'ham': 296,'lord': 202,'haue': 157,'let': 100,'good': 98,'thou': 97,'hamlet': 92,'hor': 88,'thy': 86,'enter': 68,'selfe': 64,'tis': 62,'loue': 57,'ile': 54,'giue': 52,'hath': 49,'come': 49,'thee': 48,'sir': 48,'laer': 48,



```python
import matplotlib.pyplot as plt
%matplotlib inline

# 정렬된 단어 리스트에 대해 빈도수를 가져와서 리스트 생성
w = [hamlet_word_count[key] for key in sorted_word_count]
plt.plot(w)
plt.show()
# 가독성은 부족하지만, 빈도수에 따라 정렬된 단어의 순위와 빈도수가 극단적으로 반비례함을 보여줌

# 지프의 법칙: 말뭉치의 단어들을 사용 빈도가 높은 순서대로 나열하면 단어의 사용 빈도는 단어의 순위에 반비례한다.
# 이 법칙은 언어와 관련없는 도시의 인구순위, 기업의 크기, 소득 순위와 같은 분야에서도 적용된다.

```

![image](https://user-images.githubusercontent.com/52664532/169548642-90f51e85-8b44-49ca-b08f-a40995f563a4.png)


```python
n = sorted_word_count[:20][::-1] # 빈도수 상위 20개의 단어를 추출해 역순으로 정렬
w = [hamlet_word_count[key] for key in n] # 20개 단어에 대한 빈도
plt.barh(range(len(n)), w, tick_label=n) # 수평 막대그래프
plt.show()

```
![image](https://user-images.githubusercontent.com/52664532/169657214-5cfc3245-9abb-4d3b-86ab-a35bcac692b2.png)

### 3.2 워드 클라우드로 내용을 한눈에 보기

```python

from wordcloud import WordCloud
# 워드 클라우드 이미지 생성
wordcloud = WordCloud().generate(doc_hamlet)

plt.axis("off")
plt.imshow(wordcloud, interpolation='bilinear') # 이미지를 출력
plt.show()

```
![image](https://user-images.githubusercontent.com/52664532/169657240-9a3ac84b-1982-4209-b7d8-909e409551cf.png)


```python
wordcloud.to_array().shape

```
    (200, 400, 3)


```python
wordcloud = WordCloud(max_font_size=60).generate_from_frequencies(hamlet_word_count)
plt.figure()
plt.axis("off")
plt.imshow(wordcloud, interpolation='bilinear')
plt.show()

```

![image](https://user-images.githubusercontent.com/52664532/169657278-3cd4785f-1e12-4b20-bc7a-f4d37a5cc076.png)



```python
from konlpy.corpus import kolaw
const_doc = kolaw.open('constitution.txt').read()

print(type(const_doc)) # 가져온 데이터의 type을 확인
print(len(const_doc))
print(const_doc[:600])

```

    <class 'str'>
    18884
    대한민국헌법

    유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.
    제1장 총강
    제1조 ① 대한민국은 민주공화국이다.
    ②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.
    제2조 ① 대한민국의 국민이 되는 요건은 법률로 정한다.
    ②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.
    제3조 대한민


```python
from konlpy.tag import Okt
t = Okt()
tokens_const = t.morphs(const_doc) # 형태소 단위로 tokenize

print('#토큰의 수:', len(tokens_const))
print('#앞 100개의 토큰')
print(tokens_const[:100])

```
    #토큰의 수: 8796
    #앞 100개의 토큰
    ['대한민국', '헌법', '\n\n', '유구', '한', '역사', '와', '전통', '에', '빛나는', '우리', '대', '한', '국민', '은', '3', '·', '1', '운동', '으로', '건립', '된', '대한민국', '임시정부', '의', '법', '통과', '불의', '에', '항거', '한', '4', '·', '19', '민주', '이념', '을', '계승', '하고', ',', '조국', '의', '민주', '개혁', '과', '평화', '적', '통일', '의', '사명', '에', '입', '각하', '여', '정의', '·', '인도', '와', '동포', '애', '로써', '민족', '의', '단결', '을', '공고', '히', '하고', ',', '모든', '사회', '적', '폐습', '과', '불의', '를', '타파', '하며', ',', '자율', '과', '조화', '를', '바탕', '으로', '자유민주', '적', '기', '본', '질서', '를', '더욱', '확고히', '하여', '정치', '·', '경제', '·', '사회', '·']

```python
# 일반적으로 워드클라우드는 명사만 사용해서 그림
tokens_const= t.nouns(const_doc) # 형태소 단위로 tokenize한 뒤 명사만 추출

print('#토큰의 수:', len(tokens_const))
print('#앞 100개의 토큰')
print(tokens_const[:100])

```
    #토큰의 수: 3882
    #앞 100개의 토큰
    ['대한민국', '헌법', '유구', '역사', '전통', '우리', '국민', '운동', '건립', '대한민국', '임시정부', '법', '통과', '불의', '항거', '민주', '이념', '계승', '조국', '민주', '개혁', '평화', '통일', '사명', '입', '각하', '정의', '인도', '동포', '애', '로써', '민족', '단결', '공고', '모든', '사회', '폐습', '불의', '타파', '자율', '조화', '바탕', '자유민주', '질서', '더욱', '정치', '경제', '사회', '문화', '모든', '영역', '각인', '기회', '능력', '최고', '도로', '발휘', '자유', '권리', '책임', '의무', '완수', '안', '국민', '생활', '향상', '기하', '밖', '항구', '세계', '평화', '인류', '공영', '이바지', '함', '우리', '우리', '자손', '안전', '자유', '행복', '확보', '것', '다짐', '제정', '차', '개정', '헌법', '이제', '국회', '의결', '국민투표', '개정', '제', '장', '강', '제', '대한민국', '민주공화국', '대한민국']

```python
tokens_const = [token for token in tokens_const if len(token)>1]

print('#토큰의 수:', len(tokens_const))
print('#앞 100개의 토큰')
print(tokens_const[:100])

```
    #토큰의 수: 3013
    #앞 100개의 토큰
    ['대한민국', '헌법', '유구', '역사', '전통', '우리', '국민', '운동', '건립', '대한민국', '임시정부', '통과', '불의', '항거', '민주', '이념', '계승', '조국', '민주', '개혁', '평화', '통일', '사명', '각하', '정의', '인도', '동포', '로써', '민족', '단결', '공고', '모든', '사회', '폐습', '불의', '타파', '자율', '조화', '바탕', '자유민주', '질서', '더욱', '정치', '경제', '사회', '문화', '모든', '영역', '각인', '기회', '능력', '최고', '도로', '발휘', '자유', '권리', '책임', '의무', '완수', '국민', '생활', '향상', '기하', '항구', '세계', '평화', '인류', '공영', '이바지', '우리', '우리', '자손', '안전', '자유', '행복', '확보', '다짐', '제정', '개정', '헌법', '이제', '국회', '의결', '국민투표', '개정', '대한민국', '민주공화국', '대한민국', '주권', '국민', '모든', '권력', '국민', '대한민국', '국민', '요건', '법률', '국가', '법률', '재외국민']

```python
from matplotlib import font_manager, rc

# 별도 폰트 설정도 가능
# font_path='C:/Users/xnsk2/Downloads/SpoqaHanSansNeo_all/SpoqaHanSansNeo_all/SpoqaHanSansNeo_TTF_original/SpoqaHanSansNeo-Medium.ttf'
# font_name = font_manager.FontProperties(fname=font_path).get_name()
# rc('font', family = font_name)

rc('font', family='Malgun Gothic')

const_cnt = {}
for word in tokens_const:
    const_cnt[word] = const_cnt.get(word,0)+1
    
def word_graph(cnt, max_words=10):
    sorted_w = sorted(cnt.items(), key=lambda kv:kv[1])
    print(sorted_w[-max_words:10])
    n,w = zip(*sorted_w[-max_words:])
    
    plt.barh(range(len(n)), w, tick_label=n)
    plt.show()
    
word_graph(const_cnt, max_words=20)
```
![image](https://user-images.githubusercontent.com/52664532/169672626-8b4b66a9-f1d9-4f58-a9e4-81ab9725adbb.png)
    

```python
from wordcloud import WordCloud


ont_path = '/Users/xnsk2/Downloads/SpoqaHanSansNeo_all/SpoqaHanSansNeo_all/SpoqaHanSansNeo_TTF_original/SpoqaHanSansNeo-Medium.ttf'

wordcloud = WordCloud(font_path = font_path).generate(const_doc)
plt.axis("off")
plt.imshow(wordcloud, interpolation='bilinear')
plt.show()
```
![image](https://user-images.githubusercontent.com/52664532/169672632-70e949a4-ecaf-454d-914d-2c46a067bb09.png)

```python
wordcloud = WordCloud(font_path = font_path,
                     max_font_size = 100,
                     width = 800, #이미지 너비 지정
                     height = 400, #이미지 높이 지정
                     background_color='white', #이미지 배경색 지정
                     max_words=50).generate(const_doc)

# 원문이 아닌 형태소 분석 결과로부터 워드 클라우드를 생성
wordcloud.generate_from_frequencies(const_cnt)
wordcloud.to_file('const.png') #생성한 이미지를 파일로 저장

plt.axis("off")
plt.imshow(wordcloud, interpolation='bilinear')
plt.show()

```
![image](https://user-images.githubusercontent.com/52664532/169672638-435861e1-c896-484d-990f-58aba2bf8de5.png)
