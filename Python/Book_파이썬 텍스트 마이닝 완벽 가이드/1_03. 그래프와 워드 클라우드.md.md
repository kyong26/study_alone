# 파이썬 텍스트 마이닝 완벽 가이드

## 1부. 텍스트 마이닝 기초

### 03. 그래프와 워드 클라우드

### 3.1 단어 빈도 그래프


```python
import nltk
nltk.download('gutenberg')

from nltk.corpus import gutenberg
file_names = gutenberg.fileids()

print(file_names)
```

    ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
    

    [nltk_data] Downloading package gutenberg to
    [nltk_data]     C:\Users\xnsk2\AppData\Roaming\nltk_data...
    [nltk_data]   Package gutenberg is already up-to-date!
    


```python
doc_hamlet = gutenberg.open('shakespeare-hamlet.txt').read()
print('#Num of characters used:', len(doc_hamlet)) # 사용된 문자의 수 
print('#Text sample:')
print(doc_hamlet[:500]) # 앞의 500자만 출력
```

    #Num of characters used: 162881
    #Text sample:
    [The Tragedie of Hamlet by William Shakespeare 1599]
    
    
    Actus Primus. Scoena Prima.
    
    Enter Barnardo and Francisco two Centinels.
    
      Barnardo. Who's there?
      Fran. Nay answer me: Stand & vnfold
    your selfe
    
       Bar. Long liue the King
    
       Fran. Barnardo?
      Bar. He
    
       Fran. You come most carefully vpon your houre
    
       Bar. 'Tis now strook twelue, get thee to bed Francisco
    
       Fran. For this releefe much thankes: 'Tis bitter cold,
    And I am sicke at heart
    
       Barn. Haue you had quiet Guard?
      Fran. Not
    


```python
from nltk.tokenize import word_tokenize
tokens_hamlet = word_tokenize(doc_hamlet) # 토큰화 실행

print("#Nums of tokens used:", len(tokens_hamlet))
print('#Token sample:')
print(tokens_hamlet[:20])
```

    #Nums of tokens used: 36372
    #Token sample:
    ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']
    


```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

# 모든 토큰에 대해 스테밍 실행
stem_tokens_hamlet = [stemmer.stem(token) for token in tokens_hamlet]

print("#Num of tokens after stemming:", len(stem_tokens_hampet))
print('#Token sample')
print(stem_tokens_hamlet[:20])
```

    #Num of tokens after stemming: 36372
    #Token sample
    ['[', 'the', 'tragedi', 'of', 'hamlet', 'by', 'william', 'shakespear', '1599', ']', 'actu', 'primu', '.', 'scoena', 'prima', '.', 'enter', 'barnardo', 'and', 'francisco']
    


```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# 모든 토큰에 대해 스테밍 실행
lem_tokens_hamlet = [lemmatizer.lemmatize(token) for token in tokens_hamlet]

print('#Num of tokens after lemmatization:', len(lem_tokens_hamlet))
print('#Token sample:')
print(lem_tokens_hamlet[:20])

# 어간 추출이든 표제어 추출이든 토큰 수는 변하지 않는다.
# 토큰화한 결과에 따라 개별적으로 어간 추출과 표제어 추출을 수행하기 때문이다.
```

    #Num of tokens after lemmatization: 36372
    #Token sample:
    ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']
    


```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w]{3,}")

reg_tokens_hamlet = tokenizer.tokenize(doc_hamlet.lower())
print('#Num of tokens with RegexTokenizer:', len(reg_tokens_hamlet))
print('#Token sample:')
print(reg_tokens_hamlet[:20])

# 부호와 2자 이하의 단어가 제외되어 토큰 개수가 줄었다.
# 부호는 목적에 따라 의미를 가질 수도 있고 아닐 수도 있어, 부호를 포함할지는 텍스트 마이닝 수행자에 달려 있다.
# RegexpTokenizer 사용, 시각화 결과 이해를 위해 스테밍은 하지 않기로 함
```

    #Num of tokens with RegexTokenizer: 23187
    #Token sample:
    ['the', 'tragedie', 'hamlet', 'william', 'shakespeare', '1599', 'actus', 'primus', 'scoena', 'prima', 'enter', 'barnardo', 'and', 'francisco', 'two', 'centinels', 'barnardo', 'who', 'there', 'fran']
    


```python
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들
english_stops = set(stopwords.words('english')) # 반복되지 않도록 set으로 변환

# stopwords를 제외한 단어들로만 리스트를 생성
result_hamlet = [word for word in reg_tokens_hamlet if word not in english_stops]

print('#Num of tokens after stopword elimination:', len(result_hamlet))
print('#Token sample:')
print(result_hamlet[:20])
```
    #Num of tokens after stopword elimination: 15410
    #Token sample:
    ['tragedie', 'hamlet', 'william', 'shakespeare', '1599', 'actus', 'primus', 'scoena', 'prima', 'enter', 'barnardo', 'francisco', 'two', 'centinels', 'barnardo', 'fran', 'nay', 'answer', 'stand', 'vnfold']

```python
hamlet_word_count = dict()
for word in result_hamlet:
    hamlet_word_count[word] = hamlet_word_count.get(word,0)+1
    
print('#Num of used words:', len(hamlet_word_count))

sorted_word_count = sorted(hamlet_word_count, key=hamlet_word_count.get, reverse=True)

print('#Top 20 high frequency words:')
for key in sorted_word_count[:20]: # 빈도수 상위 20개의 단어를 출력
    print(f'{repr(key)}: {hamlet_word_count[key]}', end=',')
```

    #Num of used words: 4561
    #Top 20 high frequency words:
    'ham': 337,'lord': 211,'haue': 178,'king': 172,'thou': 107,'shall': 107,'come': 104,'let': 104,'hamlet': 100,'good': 98,'hor': 95,'thy': 90,'enter': 85,'like': 80,'would': 73,'well': 71,'know': 71,'tis': 69,'selfe': 68,'loue': 66,



```python
my_tag_set = ['NN','VB','VBD','JJ']
my_words = [word for word, tag in nltk.pos_tag(result_hamlet) if tag in my_tag_set]

hamlet_word_count = dict()
for word in my_words:
    hamlet_word_count[word] = hamlet_word_count.get(word, 0)+1
    
print('#Num of used words:', len(hamlet_word_count))

sorted_word_count = sorted(hamlet_word_count, key=hamlet_word_count.get, reverse=True) #dictionary 값으로 sorting

print('#Top 20 high frequency words:')
for key in sorted_word_count[:20]: # 빈도수 상위 20개의 단어를 출력
    print(f'{repr(key)}: {hamlet_word_count[key]}', end=',')
```

    #Num of used words: 3114
    #Top 20 high frequency words:
    'ham': 296,'lord': 202,'haue': 157,'let': 100,'good': 98,'thou': 97,'hamlet': 92,'hor': 88,'thy': 86,'enter': 68,'selfe': 64,'tis': 62,'loue': 57,'ile': 54,'giue': 52,'hath': 49,'come': 49,'thee': 48,'sir': 48,'laer': 48,



```python
import matplotlib.pyplot as plt
%matplotlib inline

# 정렬된 단어 리스트에 대해 빈도수를 가져와서 리스트 생성
w = [hamlet_word_count[key] for key in sorted_word_count]
plt.plot(w)
plt.show()
# 가독성은 부족하지만, 빈도수에 따라 정렬된 단어의 순위와 빈도수가 극단적으로 반비례함을 보여줌

# 지프의 법칙: 말뭉치의 단어들을 사용 빈도가 높은 순서대로 나열하면 단어의 사용 빈도는 단어의 순위에 반비례한다.
# 이 법칙은 언어와 관련없는 도시의 인구순위, 기업의 크기, 소득 순위와 같은 분야에서도 적용된다.

```

![image](https://user-images.githubusercontent.com/52664532/169548642-90f51e85-8b44-49ca-b08f-a40995f563a4.png)


