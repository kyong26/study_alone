# 파이썬 텍스트 마이닝 완벽 가이드

## 1부. 텍스트 마이닝 기초

### 04. 카운트 기반의 문서 표현

### 4.1 카운트 기반 문서 표현 개념

- BOW(Bag of Words): 텍스트는 우리가 정의한 특성에 대한 특성 값의 집합(혹은 벡터)으로 변환하는데, 카운트 기반의 문서 표현에서는 단어가 특성이 되고, 단어의 빈도가 특성의 값이 된다. 가방에 넣으면 순서가 사라진다.  
- 대부분의 값이 0인 특성 벡터를 희소 벡터(Sparse vector)라 한다.


### 4.2 카운트 기반 문서 표현 개념

- nltk책: https://www.nltk.org/book/ch02.html



```python
import nltk
nltk.download('movie_reviews')

from nltk.corpus import movie_reviews
print("#review count:", len(movie_reviews.fileids())) #영화 리뷰 문서의 id을 반환
print("#samples of file ids:", movie_reviews.fileids()[:10]) #id를 10개까지만 출력
```

    #review count: 2000
    #samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']


### 주요함수

- fileids(): 영화 리뷰 문서들의 id를 반환한다. 매개변수 categories를 이용하면 특정 분류에 속하는 문서들의 id만 가져올 수 있다.
- categories(): 리뷰 문서들에 대한 분류, 즉 라벨을 보여준다. 여기서는 감성을 표현하는 긍정('pos')과 부정('neg') 값을 갖는다.
- raw(): 리뷰 문서의 원문을 문자열의 리스트 형태로 반환한다. 인수로 fileid를 주면 특정 문서만 가져올 수 있다.
- sents(): 리뷰 문서의 원문에 대해 NLTK의 sent_tokenize로 토큰화한 문장을 다시 word_tokenize로 토큰화한 결과를 반환한다. 인수로 fileid를 주면 특정 문서에 대한 토큰화 결과를 가져올 수 있다.
- words(): 리뷰 문서의 원문에 대해 NLTK의 word_tokenize로 토큰화한 결과를 반환한다. 인수로 fileid를 주면 특정 문서에 대한 토큰화 결과를 가져올 수 있다.


```python
fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환

print('#id of the first review:', fileid)

# 첫번째 문서의 내용을 200자까지만 출력
print('#first review content:\n', movie_reviews.raw(fileid)[:200])

# 첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장
print('#sentence tokenization result:', movie_reviews.sents(fileid)[:2])

# 첫번째 문서를 word tokenize한 결과 중 앞 20개 단어
print('#word tokenization result:', movie_reviews.words(fileid)[:20])
```

    #id of the first review: neg/cv000_29416.txt
    #first review content:
     plot : two teen couples go to a church party , drink and then drive . 
    they get into an accident . 
    one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . 
    w
    #sentence tokenization result: [['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]
    #word tokenization result: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']



```python
# 1. 텍스트 전처리를 수행해 의미가 있는 최소 단위의 리스트로 변환한다 -> words()의 기능을 사용한다.
# 2. 특성 추출 대상이 되는 단어 집합, 즉 특성 집합을 구성한다.
# 3. 각 문서별로 특성 추출 대상 단어들에 대해 단어의 빈도를 계산해 특성 벡터를 추출한다.

# 보통 텍스트 마이닝 과정에서는 사이킷런과 같은 라이브러리를 사용하나, BOW에 대한 명확한 이해를 위해 직접 구현한다.

documents = list(movie_reviews.words(fileid) for fileid in movie_reviews.fileids())
print(documents[0][:50]) # 첫째 문서의 앞 50개 단어를 출력
```

    ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', "'", 's', 'the', 'deal', '?', 'watch']



```python
word_count = {}
for text in documents:
    for word in text:
        word_count[word] = word_count.get(word, 0)+1
        
sorted_features = sorted(word_count, key=word_count.get, reverse=True)
for word in sorted_features[:10]:
    print(f"count of '{word}': {word_count[word]}", end=', ')
```

    count of ',': 77717, count of 'the': 76529, count of '.': 65876, count of 'a': 38106, count of 'and': 35576, count of 'of': 34123, count of 'to': 31937, count of ''': 30585, count of 'is': 25195, count of 'in': 21822, 


```python
# 쓸모 없어 보이는 단어들을 정규화를 통해 토큰화하고, 앞의 과정을 반복한다
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들

tokenizer = RegexpTokenizer("[\w']{3,}") # 정규표현식으로 토크나이저를 정의
english_stops = set(stopwords.words('english')) # 영어 불용어를 가져옴

# words() 대신 raw()로 원문을 가져옴
documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]

# stopwords의 적용과 토큰화를 동시에 수행
tokens = [[token for token in tokenizer.tokenize(doc) if token not in english_stops] for doc in documents]
word_count = {}
for text in tokens:
    for word in text:
        word_count[word] = word_count.get(word, 0)+1
        
sorted_features = sorted(word_count, key=word_count.get, reverse=True)

print('num of features:', len(sorted_features))
for word in sorted_features[:10]:
    print(f"count of '{word}': {word_count[word]}", end=', ')
```

    num of features: 43030
    count of 'film': 8935, count of 'one': 5791, count of 'movie': 5538, count of 'like': 3690, count of 'even': 2564, count of 'time': 2409, count of 'good': 2407, count of 'story': 2136, count of 'would': 2084, count of 'much': 2049, 


```python

```


```python

```


```python

```
