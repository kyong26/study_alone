# 파이썬 텍스트 마이닝 완벽 가이드

## 1부. 텍스트 마이닝 기초

### 04. 카운트 기반의 문서 표현

### 4.1 카운트 기반 문서 표현 개념

- BOW(Bag of Words): 텍스트는 우리가 정의한 특성에 대한 특성 값의 집합(혹은 벡터)으로 변환하는데, 카운트 기반의 문서 표현에서는 단어가 특성이 되고, 단어의 빈도가 특성의 값이 된다. 가방에 넣으면 순서가 사라진다.  
- 대부분의 값이 0인 특성 벡터를 희소 벡터(Sparse vector)라 한다.


### 4.2 카운트 기반 문서 표현 개념

- nltk책: https://www.nltk.org/book/ch02.html



```python
import nltk
nltk.download('movie_reviews')

from nltk.corpus import movie_reviews
print("#review count:", len(movie_reviews.fileids())) #영화 리뷰 문서의 id을 반환
print("#samples of file ids:", movie_reviews.fileids()[:10]) #id를 10개까지만 출력
```

    #review count: 2000
    #samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']


    [nltk_data] Downloading package movie_reviews to
    [nltk_data]     /Users/seomingyeong/nltk_data...
    [nltk_data]   Package movie_reviews is already up-to-date!


### 주요함수

- fileids(): 영화 리뷰 문서들의 id를 반환한다. 매개변수 categories를 이용하면 특정 분류에 속하는 문서들의 id만 가져올 수 있다.
- categories(): 리뷰 문서들에 대한 분류, 즉 라벨을 보여준다. 여기서는 감성을 표현하는 긍정('pos')과 부정('neg') 값을 갖는다.
- raw(): 리뷰 문서의 원문을 문자열의 리스트 형태로 반환한다. 인수로 fileid를 주면 특정 문서만 가져올 수 있다.
- sents(): 리뷰 문서의 원문에 대해 NLTK의 sent_tokenize로 토큰화한 문장을 다시 word_tokenize로 토큰화한 결과를 반환한다. 인수로 fileid를 주면 특정 문서에 대한 토큰화 결과를 가져올 수 있다.
- words(): 리뷰 문서의 원문에 대해 NLTK의 word_tokenize로 토큰화한 결과를 반환한다. 인수로 fileid를 주면 특정 문서에 대한 토큰화 결과를 가져올 수 있다.


```python
fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환

print('#id of the first review:', fileid)

# 첫번째 문서의 내용을 200자까지만 출력
print('#first review content:\n', movie_reviews.raw(fileid)[:200])

# 첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장
print('#sentence tokenization result:', movie_reviews.sents(fileid)[:2])

# 첫번째 문서를 word tokenize한 결과 중 앞 20개 단어
print('#word tokenization result:', movie_reviews.words(fileid)[:20])
```

    #id of the first review: neg/cv000_29416.txt
    #first review content:
     plot : two teen couples go to a church party , drink and then drive . 
    they get into an accident . 
    one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . 
    w
    #sentence tokenization result: [['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]
    #word tokenization result: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']



```python
# 1. 텍스트 전처리를 수행해 의미가 있는 최소 단위의 리스트로 변환한다 -> words()의 기능을 사용한다.
# 2. 특성 추출 대상이 되는 단어 집합, 즉 특성 집합을 구성한다.
# 3. 각 문서별로 특성 추출 대상 단어들에 대해 단어의 빈도를 계산해 특성 벡터를 추출한다.

# 보통 텍스트 마이닝 과정에서는 사이킷런과 같은 라이브러리를 사용하나, BOW에 대한 명확한 이해를 위해 직접 구현한다.

documents = list(movie_reviews.words(fileid) for fileid in movie_reviews.fileids())
print(documents[0][:50]) # 첫째 문서의 앞 50개 단어를 출력
```

    ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', "'", 's', 'the', 'deal', '?', 'watch']



```python
word_count = {}
for text in documents:
    for word in text:
        word_count[word] = word_count.get(word, 0)+1
        
sorted_features = sorted(word_count, key=word_count.get, reverse=True)
for word in sorted_features[:10]:
    print(f"count of '{word}': {word_count[word]}", end=', ')
```

    count of ',': 77717, count of 'the': 76529, count of '.': 65876, count of 'a': 38106, count of 'and': 35576, count of 'of': 34123, count of 'to': 31937, count of ''': 30585, count of 'is': 25195, count of 'in': 21822, 


```python
# 쓸모 없어 보이는 단어들을 정규화를 통해 토큰화하고, 앞의 과정을 반복한다
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들

tokenizer = RegexpTokenizer("[\w']{3,}") # 정규표현식으로 토크나이저를 정의
english_stops = set(stopwords.words('english')) # 영어 불용어를 가져옴

# words() 대신 raw()로 원문을 가져옴
documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]

# stopwords의 적용과 토큰화를 동시에 수행
tokens = [[token for token in tokenizer.tokenize(doc) if token not in english_stops] for doc in documents]
word_count = {}
for text in tokens:
    for word in text:
        word_count[word] = word_count.get(word, 0)+1
        
sorted_features = sorted(word_count, key=word_count.get, reverse=True)

print('num of features:', len(sorted_features))
for word in sorted_features[:10]:
    print(f"count of '{word}': {word_count[word]}", end=', ')
```

    num of features: 43030
    count of 'film': 8935, count of 'one': 5791, count of 'movie': 5538, count of 'like': 3690, count of 'even': 2564, count of 'time': 2409, count of 'good': 2407, count of 'story': 2136, count of 'would': 2084, count of 'much': 2049, 


```python
# 빈도가 높은 상위 1000개의 단어만 추출해 features를 구성
word_features = sorted_features[:1000]
```


```python
# 주어진 document를 feature로 변환하는 함수, word_features를 사용
def document_features(document, word_features):
    word_count = {}
    for word in document: #document에 있는 단어들의 빈도수를 먼저 계산
        word_count[word] = word_count.get(word, 0)+1
        
    features = []
    # word_features의 단어에 대해 계산된 빈도수를 feature에 추가
    for word in word_features:
        features.append(word_count.get(word, 0)) #빈도가 없는 단어는 0을 입력
    return features

word_features_ex = ['one', 'two', 'teen', 'couples', 'solo']
doc_ex = ['two', 'two',' couples']
print(document_features(doc_ex, word_features_ex))
```

    [0, 2, 0, 0, 0]



```python
feature_sets = [document_features(d, word_features) for d in tokens]

# 첫째 feature set의 내용을 앞 20개만 word_features의 단어와 함께 출력
for i in range(20):
    print(f'({word_features[i]}, {feature_sets[0][i]})', end=',')
```

    (film, 5),(one, 3),(movie, 6),(like, 3),(even, 3),(time, 0),(good, 2),(story, 0),(would, 1),(much, 0),(also, 1),(get, 3),(character, 1),(two, 2),(well, 1),(first, 0),(characters, 1),(see, 2),(way, 3),(make, 5),


```python
print(feature_sets[0][-20:]) # feature set의 뒤 20개만 출력
```

    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


### 4.3 사이킷런으로 카운트 벡터 생성


#### CounterVectorizer()의 주요 매개변수

- tokenizer: 함수 형태로 외부 토크나이저를 지정한다. 지정하지 않으면 자체 토크나이저를 사용한다.  
- stop_words: 리스트 형태로 불용어 사전을 지정한다. 'english'로 값을 주면 자체 영어 불용어 사전을 사용한다.  
- ngram_range: (min_n, max_n)의 튜플 형태로 ngram의 범위를 지정한다. 기본값은 (1,1)이다.  
- max_df: 단어로 특성을 구성할 때, 문서에 나타난 빈도(document frequency)가 max_df보다 크면 제외한다. 비율 혹은 문서의 수로 지정 가능하다.
- min_df: 단어로 특성을 구성할 때, 문서에 나타난 빈도(document frequency)가 min_df보다 작으면 제외한다. 비율 혹은 문서의 수로 지정 가능하다.
- max_features: 최대 특성의 수를 지정한다. 지정하지 않으면 전체 단어를 다 사용한다.
- vocabulary: 특성으로 사용할 단어들을 직접 지정한다.
- binary: True값을 주면 빈도 대신 1절에서 배운 단어의 유무(1/0)로 특성 값을 생성한다.

#### CounterVectorizer 클래스의 주요 메서드

- fit(raw_documents): 인수로 주어진 문서 집합(raw_documents)에 대해 토큰화를 수행하고 특성 집합을 생성한다.
- transform(raw_documents): fit()에서 생성한 특성 집합을 이용해 인수로 주어진 문서집합(raw_documents)에 대해 카운트벡터로 변환해 반환단다.
- fit_transform(raw_documents): 인수로 주어진 문서 집합(raw_documents)에 대해 fit과 transform을 동시에 수행한다
- get_feature_name_out(): 특성 집합에 있는 특성의 이름, 즉 단어를 순서대로 반환한다. Sklearn의 버전이 1.0으로 바뀌기 전에는 get_feature_names()였으므로 sklearn의 버전을 확인하고 이에 맞춰서 사용한다.

사이킷런은 자체적인 토크나이저를 지원하므로, 사용자가 별도로 미리 토큰화를 하지 않아도 된다.  
다만 좀 더 세부적인 조정을 통해 성능을 높이고 싶을 때는 토크나이저를 함수로 정의하고 사이킷런에서 이를 사용할 수 있다. 한글의 경우에는 KoNLPy로 형태소 분석을 수행해야 하므로 반드시 별도의 토크나이저를 사용해야 한다.


```python
# data 준비, movie_reviews_raw()를 사용해 raw text를 추출
reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]
```


```python
from sklearn.feature_extraction.text import CountVectorizer

#cv = CountVectorizer() #모든 매개변수에 기본값을 사용하는 경우

#앞에서 생성한 word_features를 특성 집합을 지정하는 경우
cv = CountVectorizer(vocabulary=word_features)

#특성 집합을 지정하지 않고 최대 특성의 수를 지정하는 경우
#cv = CountVectorizer(max_features=1000)

print(cv) #객체의 인수를 확인
```

    CountVectorizer(vocabulary=['film', 'one', 'movie', 'like', 'even', 'time',
                                'good', 'story', 'would', 'much', 'also', 'get',
                                'character', 'two', 'well', 'first', 'characters',
                                'see', 'way', 'make', 'life', 'really', 'films',
                                'plot', 'little', 'people', 'could', 'bad', 'scene',
                                'never', ...])



```python
reviews_cv = cv.fit_transform(reviews) #reviews를 이용해 count vector를 학습하고 변환
print(cv.get_feature_names_out()[:20]) #count vector에 사용된 feature 이름을 반환
print(word_features[:20]) #비교를 위해 출력
```

    ['film' 'one' 'movie' 'like' 'even' 'time' 'good' 'story' 'would' 'much'
     'also' 'get' 'character' 'two' 'well' 'first' 'characters' 'see' 'way'
     'make']
    ['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']



```python
print('#type of count vectors', type(reviews_cv))
print('#shape of count vectors', reviews_cv.shape)
print('#sample of count vectors:')
print(reviews_cv[0,:10])
```

    #type of count vectors <class 'scipy.sparse.csr.csr_matrix'>
    #shape of count vectors (2000, 1000)
    #sample of count vectors:
      (0, 0)	6
      (0, 1)	3
      (0, 2)	6
      (0, 3)	3
      (0, 4)	3
      (0, 6)	2
      (0, 8)	1



```python
reviews_cv
```




    <2000x1000 sparse matrix of type '<class 'numpy.int64'>'
    	with 252984 stored elements in Compressed Sparse Row format>




```python
print(feature_sets[0][:20]) #앞에서 직접 계산한 카운트 벡터

# 변환된 결과의 첫째 feature set중에서 앞 20개를 출력
print(reviews_cv.toarray()[0][0:20])
```

    [5, 3, 6, 3, 3, 0, 2, 0, 1, 0, 1, 3, 1, 2, 1, 0, 1, 2, 3, 5]
    [6 3 6 3 3 0 2 0 1 0 1 3 2 2 1 0 1 2 3 5]



```python
for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv.toarray()[0][0:20]):
    print(f'{word}:{count}', end=', ')
```

    film:6, one:3, movie:6, like:3, even:3, time:0, good:2, story:0, would:1, much:0, also:1, get:3, character:2, two:2, well:1, first:0, characters:1, see:2, way:3, make:5, 


```python

```
