# 파이썬 텍스트 마이닝 완벽 가이드

## 1부. 텍스트 마이닝 기초

### 03. 그래프와 워드 클라우드

### 3.1 단어 빈도 그래프


```python
import nltk
nltk.download('gutenberg')

from nltk.corpus import gutenberg
file_names = gutenberg.fileids()

print(file_names)
```

    ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
    

    [nltk_data] Downloading package gutenberg to
    [nltk_data]     C:\Users\xnsk2\AppData\Roaming\nltk_data...
    [nltk_data]   Package gutenberg is already up-to-date!
    


```python
doc_hamlet = gutenberg.open('shakespeare-hamlet.txt').read()
print('#Num of characters used:', len(doc_hamlet)) # 사용된 문자의 수 
print('#Text sample:')
print(doc_hamlet[:500]) # 앞의 500자만 출력
```

    #Num of characters used: 162881
    #Text sample:
    [The Tragedie of Hamlet by William Shakespeare 1599]
    
    
    Actus Primus. Scoena Prima.
    
    Enter Barnardo and Francisco two Centinels.
    
      Barnardo. Who's there?
      Fran. Nay answer me: Stand & vnfold
    your selfe
    
       Bar. Long liue the King
    
       Fran. Barnardo?
      Bar. He
    
       Fran. You come most carefully vpon your houre
    
       Bar. 'Tis now strook twelue, get thee to bed Francisco
    
       Fran. For this releefe much thankes: 'Tis bitter cold,
    And I am sicke at heart
    
       Barn. Haue you had quiet Guard?
      Fran. Not
    


```python
from nltk.tokenize import word_tokenize
tokens_hamlet = word_tokenize(doc_hamlet) # 토큰화 실행

print("#Nums of tokens used:", len(tokens_hamlet))
print('#Token sample:')
print(tokens_hamlet[:20])
```

    #Nums of tokens used: 36372
    #Token sample:
    ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']
    


```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

# 모든 토큰에 대해 스테밍 실행
stem_tokens_hampet = [stemmer.stem(token) for token in tokens_hamlet]

print("#Num of tokens after stemming:", len(stem_tokens_hampet))
print('#Token sample')
print(stem_tokens_alice[:20])
```

    #Num of tokens after stemming: 36372
    #Token sample
    ['[', 'the', 'tragedi', 'of', 'hamlet', 'by', 'william', 'shakespear', '1599', ']', 'actu', 'primu', '.', 'scoena', 'prima', '.', 'enter', 'barnardo', 'and', 'francisco']
    


```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# 모든 토큰에 대해 스테밍 실행
lem_tokens_hamlet = [lemmatizer.lemmatize(token) for token in tokens_hamlet]

print('#Num of tokens after lemmatization:', len(lem_tokens_hamlet))
print('#Token sample:')
print(lem_tokens_hamlet[:20])

# 어간 추출이든 표제어 추출이든 토큰 수는 변하지 않는다.
# 토큰화한 결과에 따라 개별적으로 어간 추출과 표제어 추출을 수행하기 때문이다.
```

    #Num of tokens after lemmatization: 36372
    #Token sample:
    ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Enter', 'Barnardo', 'and', 'Francisco']
    


```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w]{3,}")

reg_tokens_hamlet = tokenizer.tokenize(doc_hamlet.lower())
print('#Num of tokens with RegexTokenizer:', len(reg_tokens_hamlet))
print('#Token sample:')
print(reg_tokens_hamlet[:20])

# 부호와 2자 이하의 단어가 제외되어 토큰 개수가 줄었다.
# 부호는 목적에 따라 의미를 가질 수도 있고 아닐 수도 있어, 부호를 포함할지는 텍스트 마이닝 수행자에 달려 있다.
# RegexpTokenizer 사용, 시각화 결과 이해를 위해 스테밍은 하지 않기로 함
```

    #Num of tokens with RegexTokenizer: 23187
    #Token sample:
    ['the', 'tragedie', 'hamlet', 'william', 'shakespeare', '1599', 'actus', 'primus', 'scoena', 'prima', 'enter', 'barnardo', 'and', 'francisco', 'two', 'centinels', 'barnardo', 'who', 'there', 'fran']
    


```python

```


```python

```


```python

```


```python

```
